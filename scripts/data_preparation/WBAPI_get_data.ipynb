{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, salesforce.com, inc and MILA.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root  \n",
    "or https://opensource.org/licenses/BSD-3-Clause  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, salesforce.com, inc and MILA.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root  \n",
    "or https://opensource.org/licenses/BSD-3-Clause  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the notebook to create the datasets and yaml file\n",
    "Dependency: wbgapi, pandas, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:38:48.755024Z",
     "start_time": "2022-06-20T13:38:41.305030Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install wbgapi\n",
    "import wbgapi as wb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from opt_helper import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get convergence population predicted by United Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:38:48.878908Z",
     "start_time": "2022-06-20T13:38:48.757020Z"
    }
   },
   "outputs": [],
   "source": [
    "lasdf = pd.read_csv(\"UN-pop-pred.csv\")\n",
    "lasdf = lasdf[lasdf[\"Year\"]==2100].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the country classes information from the world bank classification. Class 0 includes countries all over the world. Class 1 to 20 are divided by region X income level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:38:48.955369Z",
     "start_time": "2022-06-20T13:38:48.883898Z"
    }
   },
   "outputs": [],
   "source": [
    "cc = pd.read_csv(\"CountryClass.csv\")\n",
    "countryclass = {i:list(cc[cc[\"RIG\"]==i][\"Code\"]) for i in range(1,21)}\n",
    "countryclass[0] = sum([countryclass[i] for i in range(1,21)] ,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create coalition yamls\n",
    "# countryclass = extract_group_memberships(\"coalition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the env protection proportion contribution from the IMF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envdf = pd.read_csv(\"Environmental_Protection_Expenditures_Geo_Avg_Recent_Years_Sum.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the tax rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxdf = pd.read_csv(\"tax_rate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the env protection expenditure for the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "env_pay = {}\n",
    "with open(\"Environmental_Protection_Expenditures_Geo_Avg_Recent_Years_Sum.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        env_pay[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series (token) what we are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:41:20.862798Z",
     "start_time": "2022-06-20T13:38:48.961353Z"
    }
   },
   "outputs": [],
   "source": [
    "# the list of series that we want to query\n",
    "series_list = [\"NY.GDP.MKTP.CD\",\"CM.MKT.LCAP.CD\", \"SP.POP.TOTL\",\"EN.ATM.CO2E.KT\", \"NE.CON.TOTL.ZS\"]\n",
    "df = wb.data.DataFrame(series_list, time=range(1960, 2022, 1), labels=True)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine which years are interested and what countries should be excluded (because of lacking in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:41:20.877725Z",
     "start_time": "2022-06-20T13:41:20.864791Z"
    }
   },
   "outputs": [],
   "source": [
    "# retrive country codes stuff and make sure the data are float rather than str\n",
    "economy_list = list(df[\"economy\"])[:len(set(df[\"economy\"]))]\n",
    "country_list = list(df[\"Country\"])[:len(set(df[\"economy\"]))]\n",
    "economy_region_list = []\n",
    "for x in economy_list:\n",
    "    if x==\"WLD\": break\n",
    "    else: economy_region_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:41:29.002601Z",
     "start_time": "2022-06-20T13:41:20.912271Z"
    }
   },
   "outputs": [],
   "source": [
    "# add to the exec_code list because no GDP data available\n",
    "noY=[]\n",
    "for x in economy_region_list:\n",
    "    tmp = []\n",
    "    for y in range(2003, 2021):\n",
    "        if pd.isnull(get_data_list(df, x, \"Y\")[1][\"YR\"+str(y)]):\n",
    "            tmp.append(0)\n",
    "        else:\n",
    "            tmp.append(1)\n",
    "    if sum(tmp)!=len(range(2003, 2021)):\n",
    "        noY.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-20T13:41:29.049095Z",
     "start_time": "2022-06-20T13:41:29.005595Z"
    }
   },
   "outputs": [],
   "source": [
    "exe_code = noY\n",
    "exc_code = ['YEM','VIR','VEN', 'TKM','SYR', 'MAF', 'SSD', 'SOM', 'SXM', 'SMR', 'MNP', 'NCL', 'NRU', 'LIE', 'XKX', 'PRK', 'IMN', \n",
    "            'GRL', 'GIB', 'PYF', 'FRO', 'ERI', 'CUW', 'CHI', 'CYM', 'VGB', 'ABW', 'AND', \"TWN\"] # exclude codes becasue we don't even have GDP data ever\n",
    "dict_country = {}\n",
    "for i in range(len(economy_list)):\n",
    "    dict_country[economy_list[i]]=country_list[i]\n",
    "years = list(df.columns)[4:]\n",
    "df[years] = df[years].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing the carbon intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:18.676076Z",
     "start_time": "2022-05-24T13:51:14.705700Z"
    }
   },
   "outputs": [],
   "source": [
    "# include the carbon intensity\n",
    "for i in range(len(country_list)):\n",
    "    s = df[df[\"economy\"]==economy_list[i]][df[\"series\"]==\"EN.ATM.CO2E.KT\"][years].reset_index(drop=True)*1_000_000/df[df[\"economy\"]==economy_list[i]][df[\"series\"]==\"NY.GDP.MKTP.CD\"][years].reset_index(drop=True)\n",
    "    s.at[0,\"economy\"] = economy_list[i]\n",
    "    s.at[0,\"Country\"] = country_list[i]\n",
    "    s.at[0,\"series\"] = \"EN.ATM.CO2E.KD.CD\"\n",
    "    s.at[0, \"Series\"] = \"sigma\"\n",
    "    df = df.append(s)\n",
    "    df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:19.181076Z",
     "start_time": "2022-05-24T13:51:18.679081Z"
    }
   },
   "outputs": [],
   "source": [
    "# check those regions who does not have sigma data\n",
    "count = 0\n",
    "noco2 = {}\n",
    "hasco2 = {}\n",
    "for i in range(len(country_list)):\n",
    "    if df[df[\"series\"]==\"EN.ATM.CO2E.KD.CD\"][df[\"economy\"]==economy_list[i]][\"YR2018\"].isnull().values[0]:\n",
    "        noco2[economy_list[i]]=country_list[i]\n",
    "        continue\n",
    "    else:\n",
    "        hasco2[economy_list[i]]=country_list[i]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:19.197124Z",
     "start_time": "2022-05-24T13:51:19.184074Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the country list without co2 data\n",
    "noco2_region_dict = {}\n",
    "for k,v in noco2.items():\n",
    "    if k==\"WLD\": break\n",
    "    else: noco2_region_dict[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:19.212122Z",
     "start_time": "2022-05-24T13:51:19.202121Z"
    }
   },
   "outputs": [],
   "source": [
    "# since there is no co2 data for the key countries, \n",
    "# we manually find a replacement data for them from other countires\n",
    "borrowdict = {'PSE': 'EGY',\n",
    " 'VIR': 'USA',\n",
    " 'VEN': 'MEX',\n",
    " 'TCA': 'GBR',\n",
    " 'MAF': 'FRA',\n",
    " 'SSD': 'EGY',\n",
    " 'SXM': 'NLD',\n",
    " 'SMR': 'ITA',\n",
    " 'PRI': 'USA',\n",
    " 'MNP': 'USA',\n",
    " 'NCL': 'FRA',\n",
    " 'MCO': 'FRA',\n",
    " 'MAC': 'CHN',\n",
    " 'XKX': 'TUR',\n",
    " 'PRK': \"RUS\",\n",
    " 'IMN': 'GBR',\n",
    " 'HKG': 'CHN',\n",
    " 'GUM': 'USA',\n",
    " 'GRL': 'DNK',\n",
    " 'GIB': 'GBR',\n",
    " 'PYF': 'FRA',\n",
    " 'FRO': 'DNK',\n",
    " 'ERI': 'EGY',\n",
    " 'CUW': 'NLD',\n",
    " 'CHI': 'GBR',\n",
    " 'CYM': 'GBR',\n",
    " 'VGB': 'GBR',\n",
    " 'BMU': 'CAN',\n",
    " 'ABW': 'MEX',\n",
    " 'ASM': 'USA',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:19.273117Z",
     "start_time": "2022-05-24T13:51:19.217122Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find the country code to index mapping for noco2data available countries\n",
    "code2idx = {}\n",
    "for k in noco2_region_dict.keys():\n",
    "    code2idx[k] = df[df[\"series\"]==\"EN.ATM.CO2E.KD.CD\"][df[\"economy\"]==k].index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:19.718638Z",
     "start_time": "2022-05-24T13:51:19.275119Z"
    }
   },
   "outputs": [],
   "source": [
    "# update the co2 data for noco2 recorded countries by borrowdict\n",
    "for k in noco2_region_dict.keys(): # borrowdict\n",
    "    try:\n",
    "        df.loc[code2idx[k],years] = df[df[\"series\"]==\"EN.ATM.CO2E.KD.CD\"][df[\"economy\"]==borrowdict[k]][years].iloc[0]\n",
    "    except:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fillna for countries without capital data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:19.733743Z",
     "start_time": "2022-05-24T13:51:19.721641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the (Y,L) data and predict K data by KNN\n",
    "def get_Y_K_L_pairs(codes, year=None, exc_code=[]):\n",
    "    if year is None:\n",
    "        year = \"YR2020\"\n",
    "    else:\n",
    "        year = \"YR\"+str(year)\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    train_codes = []\n",
    "    test_data = []\n",
    "    test_codes = []\n",
    "    codes_ = []\n",
    "    for code in codes:\n",
    "        if code in exc_code:\n",
    "            continue\n",
    "        else:\n",
    "            label = get_data_list(df, code, \"K\")[1][year]\n",
    "            if pd.isnull(label):\n",
    "                test_data.append([get_data_list(df, code, \"Y\")[1][year], get_data_list(df, code, \"L\")[1][year]])\n",
    "                test_codes.append(code)\n",
    "            else:\n",
    "                train_data.append([get_data_list(df, code, \"Y\")[1][year], get_data_list(df, code, \"L\")[1][year]])\n",
    "                train_label.append(label)\n",
    "                train_codes.append(code)\n",
    "    return np.array(train_data), np.array(train_label), np.array(test_data), dict(zip(train_codes, train_data)), dict(zip(test_codes, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:52.236323Z",
     "start_time": "2022-05-24T13:51:19.736164Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use KNN to predict those who has Y, L data but don't have K data\n",
    "for y in range(2003, 2021):\n",
    "    train_data, train_label, test_data, train_dict, test_dict = get_Y_K_L_pairs(economy_region_list, y, exc_code)\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    neigh = KNeighborsRegressor(n_neighbors=5)\n",
    "    neigh.fit(train_data, train_label)\n",
    "    test_K = neigh.predict(test_data)\n",
    "    test_region = list(test_dict.keys())\n",
    "    for i in range(len(test_region)):\n",
    "        idx = df[df[\"economy\"]==test_region[i]][df[\"series\"]==\"CM.MKT.LCAP.CD\"][\"YR2003\"].index.values[0]\n",
    "        df.loc[idx,\"YR\"+str(y)]=test_K[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use KNN to fill consumption dataï¼Œ prepare the (Y,L) data and predict C data by KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y_C_L_pairs(codes, year=None, exc_code=[]):\n",
    "    if year is None:\n",
    "        year = \"YR2020\"\n",
    "    else:\n",
    "        year = \"YR\"+str(year)\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    train_codes = []\n",
    "    test_data = []\n",
    "    test_codes = []\n",
    "    codes_ = []\n",
    "    for code in codes:\n",
    "        if code in exc_code:\n",
    "            continue\n",
    "        else:\n",
    "            label = get_data_list(df, code, \"C\")[1][year]\n",
    "            if pd.isnull(label):\n",
    "                test_data.append([get_data_list(df, code, \"Y\")[1][year], get_data_list(df, code, \"L\")[1][year]])\n",
    "                test_codes.append(code)\n",
    "            else:\n",
    "                train_data.append([get_data_list(df, code, \"Y\")[1][year], get_data_list(df, code, \"L\")[1][year]])\n",
    "                train_label.append(label)\n",
    "                train_codes.append(code)\n",
    "    return np.array(train_data), np.array(train_label), np.array(test_data), dict(zip(train_codes, train_data)), dict(zip(test_codes, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KNN to predict those who has Y, L data but don't have K data\n",
    "for y in range(2003, 2021):\n",
    "    train_data, train_label, test_data, train_dict, test_dict = get_Y_C_L_pairs(economy_region_list, y, exc_code)\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    neigh = KNeighborsRegressor(n_neighbors=5)\n",
    "    neigh.fit(train_data, train_label)\n",
    "    test_C = neigh.predict(test_data)\n",
    "    test_region = list(test_dict.keys())\n",
    "    for i in range(len(test_region)):\n",
    "        idx = df[df[\"economy\"]==test_region[i]][df[\"series\"]==\"NE.CON.TOTL.ZS\"][\"YR2003\"].index.values[0]\n",
    "        df.loc[idx,\"YR\"+str(y)]=test_C[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in economy_region_list:\n",
    "    if code in exc_code:\n",
    "        continue\n",
    "    else:\n",
    "        if len(get_data_list(df, code, \"C\")[0]) == 0:\n",
    "            print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduce the tech factor (ATFP) based on Y,K,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:56.578066Z",
     "start_time": "2022-05-24T13:51:52.240330Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(economy_list)):\n",
    "    if economy_list[i] in exc_code: continue\n",
    "    else:\n",
    "        s = df[df[\"economy\"]==economy_list[i]][df[\"series\"]==\"NY.GDP.MKTP.CD\"][years].reset_index(drop=True)/(1000_000_000_000*((df[df[\"economy\"]==economy_list[i]][df[\"series\"]==\"CM.MKT.LCAP.CD\"][years].reset_index(drop=True)/1000_000_000_000)**0.3*(df[df[\"economy\"]==economy_list[i]][df[\"series\"]==\"SP.POP.TOTL\"][years].reset_index(drop=True)/1000_000_000)**0.7))\n",
    "        s.at[0,\"economy\"] = economy_list[i]\n",
    "        s.at[0,\"Country\"] = country_list[i]\n",
    "        s.at[0,\"series\"] = \"ATFP\"\n",
    "        s.at[0, \"Series\"] = \"A\"\n",
    "        df = df.append(s)\n",
    "        df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the results of the time series accross different regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:58.650991Z",
     "start_time": "2022-05-24T13:51:56.580580Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_results = {}\n",
    "for x in economy_region_list:\n",
    "    if x in exc_code:\n",
    "        continue\n",
    "    else:\n",
    "        raw_results[x]={}\n",
    "        raw_results[x][\"TS_Y\"] = get_data_list(df, x, \"Y\")\n",
    "        raw_results[x][\"TS_A\"] = get_data_list(df, x, \"A\")\n",
    "        raw_results[x][\"TS_K\"] = get_data_list(df, x, \"K\")\n",
    "        raw_results[x][\"TS_L\"] = get_data_list(df, x, \"L\")\n",
    "        raw_results[x][\"TS_sigma\"] = get_data_list(df, x, \"sigma\")\n",
    "        raw_results[x][\"TS_C\"] = get_data_list(df, x, \"C\")\n",
    "        raw_results[x][\"La\"] = list(lasdf[lasdf[\"Code\"]==x][\"Population (future projections)\"])[0]\n",
    "        raw_results[x][\"mitigation\"] = get_env_data_list(envdf, x)\n",
    "        raw_results[x][\"saving\"] = 1 - int(raw_results[x][\"TS_C\"][0][-1])/100\n",
    "        try:\n",
    "            raw_results[x][\"tax\"] = get_tax_data_list(taxdf, x)\n",
    "        except:\n",
    "            raw_results[x][\"tax\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge regions from data from small regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the time series length of A, K and L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:58.665991Z",
     "start_time": "2022-05-24T13:51:58.652992Z"
    }
   },
   "outputs": [],
   "source": [
    "def packup_regions(raw_data, groupnum):\n",
    "    output = {}\n",
    "    starts = []\n",
    "    ends = []\n",
    "    for token in [\"TS_A\", \"TS_K\", \"TS_L\"]: \n",
    "        for c in countryclass[groupnum]:\n",
    "            if c in exc_code:\n",
    "                continue\n",
    "            starts.append(raw_data[c][token][3])\n",
    "            ends.append(raw_data[c][token][4])\n",
    "    s = max(starts)\n",
    "    e = min(ends)\n",
    "    \n",
    "    for token in [\"TS_A\", \"TS_K\", \"TS_L\"]: \n",
    "        output[token] = []\n",
    "        for c in countryclass[groupnum]:\n",
    "            if c in exc_code:\n",
    "                continue\n",
    "            for i in range(s,e+1):\n",
    "                if i==s:\n",
    "                    output[token].append([])\n",
    "                output[token][-1].append(raw_data[c][token][1][\"YR\"+str(i)])\n",
    "    output[\"La_s\"] = [raw_results[c][\"La\"] for c in countryclass[groupnum] if c not in exc_code]\n",
    "    output[\"sigmai_s\"] = [raw_results[c][\"TS_sigma\"][0][-1] for c in countryclass[groupnum] if c not in exc_code]\n",
    "    output[\"tax_s\"] = [raw_results[c][\"tax\"] for c in countryclass[groupnum] if c not in exc_code]\n",
    "    output[\"mitigation_s\"] = [raw_results[c][\"mitigation\"] for c in countryclass[groupnum] if c not in exc_code]\n",
    "    output[\"saving_s\"] = [raw_results[c][\"saving\"] for c in countryclass[groupnum] if c not in exc_code]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An output template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:58.680991Z",
     "start_time": "2022-05-24T13:51:58.670996Z"
    }
   },
   "outputs": [],
   "source": [
    "default = {\"_RICE_CONSTANT\":\n",
    "  {\"xgamma\": 0.3, # in CAP Eq 5 the capital elasticty\n",
    "\n",
    "  # A rice data\n",
    "  \"xA_0\": 0,\n",
    "  \"xg_A\": 0,\n",
    "  \"xdelta_A\": 0.0214976314392836,\n",
    "  # L\n",
    "  \"xL_0\": 1397.715000, # in POP population at the staring point\n",
    "  \"xL_a\": 1297.666000, # in POP the expected population at convergence\n",
    "  \"xl_g\": 0.04047275402855734, # in POP control the rate to converge\n",
    "  # K\n",
    "  \"xK_0\": 93.338152,\n",
    "  \"xa_1\": 0,\n",
    "  \"xa_2\": 0.00236 ,\n",
    "  \"xa_3\": 2,\n",
    "\n",
    "  # xsigma_0: 0.5201338309755572\n",
    "  \"xsigma_0\": 0.215}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the parameters of the dynamics from the timeseries gather the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:58.912990Z",
     "start_time": "2022-05-24T13:51:58.686996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "para_result = {i:{} for i in range(1,21)}\n",
    "for i in range(1,21):\n",
    "    print(countryclass[i])\n",
    "    a = merge_region_dict(packup_regions(raw_results, i))\n",
    "    para_result[i][\"xl_g\"] = get_pop_lg(a[\"Ls\"], a[\"Las\"])\n",
    "    para_result[i][\"xL_a\"] = a[\"Las\"]/1_000_000\n",
    "    para_result[i][\"xL_0\"] = a[\"Ls\"][-1]/1_000_000\n",
    "    para_result[i][\"xg_A\"],para_result[i][\"xdelta_A\"] = get_gA_deltaA(a[\"As\"])\n",
    "    para_result[i][\"xA_0\"] = a[\"As\"][-1]\n",
    "    para_result[i][\"xK_0\"] = a[\"Ks\"][-1]/1_000_000_000_000\n",
    "    para_result[i][\"xsigma_0\"] = a[\"sigmas\"]/(1-0.05)\n",
    "    para_result[i][\"xtax\"] = a[\"taxs\"]\n",
    "    para_result[i][\"xmitigation_0\"] = a[\"mitigations\"]\n",
    "    para_result[i][\"xsaving_0\"] = a[\"savings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "para_result = {k:{} for k in countryclass.keys()}\n",
    "for i in countryclass.keys():\n",
    "    print(countryclass[i])\n",
    "    a = merge_region_dict(packup_regions(raw_results, i))\n",
    "    para_result[i][\"xl_g\"] = get_pop_lg(a[\"Ls\"], a[\"Las\"])\n",
    "    para_result[i][\"xL_a\"] = a[\"Las\"]/1_000_000\n",
    "    para_result[i][\"xL_0\"] = a[\"Ls\"][-1]/1_000_000\n",
    "    para_result[i][\"xg_A\"],para_result[i][\"xdelta_A\"] = get_gA_deltaA(a[\"As\"])\n",
    "    para_result[i][\"xA_0\"] = a[\"As\"][-1]\n",
    "    para_result[i][\"xK_0\"] = a[\"Ks\"][-1]/1_000_000_000_000\n",
    "    para_result[i][\"xsigma_0\"] = a[\"sigmas\"]/(1-0.05)\n",
    "    para_result[i][\"xtax\"] = a[\"taxs\"]\n",
    "    para_result[i][\"xmitigation_0\"] = a[\"mitigations\"]\n",
    "    para_result[i][\"xsaving_0\"] = a[\"savings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the \"rest of the world\" region and used the worldwide parameters for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:58.959490Z",
     "start_time": "2022-05-24T13:51:58.934489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the number of population which is not covered\n",
    "popworldwide = list(df[df[\"economy\"]==\"WLD\"][df[\"series\"]==\"SP.POP.TOTL\"][\"YR2020\"])[0]\n",
    "popcovvered = sum([raw_results[x][\"TS_L\"][0][-1] for x in economy_region_list if x not in exc_code])\n",
    "popnotcovered = popworldwide - popcovvered\n",
    "# calculate the estimate worldwide convergence population\n",
    "covered_convergence_pop = sum([raw_results[x][\"La\"] for x in economy_region_list if x not in exc_code])\n",
    "not_covvered_convergence_pop = popnotcovered*covered_convergence_pop/popcovvered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:59.021488Z",
     "start_time": "2022-05-24T13:51:58.967494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the worldwide properties\n",
    "i=0\n",
    "wld = {}\n",
    "a = merge_region_dict(packup_regions(raw_results, i))\n",
    "wld[\"xl_g\"] = get_pop_lg(a[\"Ls\"], a[\"Las\"])\n",
    "wld[\"xL_a\"] = a[\"Las\"]/1_000_000\n",
    "wld[\"xL_0\"] = a[\"Ls\"][-1]/1_000_000\n",
    "wld[\"xg_A\"],wld[\"xdelta_A\"] = get_gA_deltaA(a[\"As\"])\n",
    "wld[\"xA_0\"] = a[\"As\"][-1]\n",
    "wld[\"xK_0\"] = a[\"Ks\"][-1]/1_000_000_000_000\n",
    "wld[\"xsigma_0\"] = a[\"sigmas\"]/(1-0.05)\n",
    "wld[\"xtax\"] = a[\"taxs\"]\n",
    "wld[\"xmitigation_0\"] = a[\"mitigations\"]\n",
    "wld[\"xsaving_0\"] = a[\"savings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:59.036491Z",
     "start_time": "2022-05-24T13:51:59.024487Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the rest of the world params\n",
    "rest_region = wld.copy()\n",
    "rest_region[\"xL_a\"] = not_covvered_convergence_pop/1_000_000\n",
    "rest_region[\"xL_0\"] = popnotcovered/1_000_000\n",
    "rest_region[\"xK_0\"] = popnotcovered*wld[\"xK_0\"]/popcovvered\n",
    "para_result[21] = rest_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split USA, CHN and IND because they are too large economics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:59.068487Z",
     "start_time": "2022-05-24T13:51:59.044490Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_region(datadict, code, start, end, splits=[], gamma=0.3):\n",
    "    A, K, L =[],[],[]\n",
    "    for i in range(start, end+1):\n",
    "        A.append(datadict[code][\"TS_A\"][1][\"YR\"+str(i)])\n",
    "        K.append(datadict[code][\"TS_K\"][1][\"YR\"+str(i)])\n",
    "        L.append(datadict[code][\"TS_L\"][1][\"YR\"+str(i)])\n",
    "    A,K,L = np.array(A),np.array(K),np.array(L)\n",
    "    La = datadict[code][\"La\"]\n",
    "    Y = A*K**gamma*L**(1-gamma)\n",
    "    splits = np.array(splits)\n",
    "    if len(splits)==0:\n",
    "        splits = np.random.rand(4)\n",
    "    if sum(splits)!=1:\n",
    "        splits = splits/sum(splits)\n",
    "    LEN = len(splits)\n",
    "    Ys = Y.reshape(1,-1)\n",
    "    Ys = np.repeat(Ys,LEN,axis=0)\n",
    "    for i in range(len(splits)):\n",
    "        Ys[i] = Ys[i]*splits[i]\n",
    "    Ls = L.reshape(1,-1)\n",
    "    Ls = np.repeat(Ls,LEN,axis=0)\n",
    "    for i in range(len(splits)):\n",
    "        Ls[i] = Ls[i]*splits[i]\n",
    "    multiplier = np.clip(np.exp(np.random.normal(0.5,0.5,LEN)), 0.75, 2)\n",
    "    As = A.reshape(1,-1)\n",
    "    As = np.repeat(As,LEN,axis=0)\n",
    "    for i in range(len(multiplier)):\n",
    "        As[i] = As[i]*multiplier[i]\n",
    "    Ks = (Ys/(As*Ls**(1-gamma)))**(1/gamma)\n",
    "    Las = La*splits\n",
    "    return As, Ks, Ls, Las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:59.099493Z",
     "start_time": "2022-05-24T13:51:59.072490Z"
    }
   },
   "outputs": [],
   "source": [
    "splitted_data = {}\n",
    "splitted_data[\"USA\"] = split_region(raw_results, \"USA\", 1975, 2020, [0.5, 0.33, 0.17])\n",
    "splitted_data[\"CHN\"] = split_region(raw_results, \"CHN\", 2003, 2020, [0.5, 0.33, 0.17])\n",
    "splitted_data[\"IND\"] = split_region(raw_results, \"IND\", 2000, 2020, [0.5, 0.33, 0.17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:51:59.252487Z",
     "start_time": "2022-05-24T13:51:59.105489Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "para_result.update({i:{} for i in range(22,31)})\n",
    "for i in range(0,3):\n",
    "    para_result[i+22][\"xl_g\"] = get_pop_lg(splitted_data[\"USA\"][2][i], splitted_data[\"USA\"][2][i])\n",
    "    para_result[i+22][\"xL_a\"] = splitted_data[\"USA\"][3][i]/1_000_000\n",
    "    para_result[i+22][\"xL_0\"] = splitted_data[\"USA\"][2][i][-1]/1_000_000\n",
    "    para_result[i+22][\"xg_A\"],para_result[i+22][\"xdelta_A\"] = get_gA_deltaA(splitted_data[\"USA\"][0][i])\n",
    "    para_result[i+22][\"xA_0\"] = splitted_data[\"USA\"][0][i][-1]\n",
    "    para_result[i+22][\"xK_0\"] = splitted_data[\"USA\"][1][i][-1]/1_000_000_000_000\n",
    "    para_result[i+22][\"xsigma_0\"] = raw_results[\"USA\"][\"TS_sigma\"][0][-1]/0.95\n",
    "    para_result[i+22][\"xtax\"] = para_result[1][\"xtax\"] # USA country code 1\n",
    "    para_result[i+22][\"xmitigation_0\"] = para_result[1][\"xmitigation_0\"]\n",
    "    para_result[i+22][\"xsaving_0\"] = para_result[1][\"xsaving_0\"]\n",
    "for i in range(0,3):\n",
    "    para_result[i+25][\"xl_g\"] = get_pop_lg(splitted_data[\"CHN\"][2][i], splitted_data[\"CHN\"][2][i])\n",
    "    para_result[i+25][\"xL_a\"] = splitted_data[\"CHN\"][3][i]/1_000_000\n",
    "    para_result[i+25][\"xL_0\"] = splitted_data[\"CHN\"][2][i][-1]/1_000_000\n",
    "    para_result[i+25][\"xg_A\"],para_result[i+25][\"xdelta_A\"] = get_gA_deltaA(splitted_data[\"CHN\"][0][i])\n",
    "    para_result[i+25][\"xA_0\"] = splitted_data[\"CHN\"][0][i][-1]\n",
    "    para_result[i+25][\"xK_0\"] = splitted_data[\"CHN\"][1][i][-1]/1_000_000_000_000\n",
    "    para_result[i+25][\"xsigma_0\"] = raw_results[\"CHN\"][\"TS_sigma\"][0][-1]/0.95\n",
    "    para_result[i+25][\"xtax\"] = para_result[8][\"xtax\"] # CHN country code 8\n",
    "    para_result[i+25][\"xmitigation_0\"] = para_result[8][\"xmitigation_0\"]\n",
    "    para_result[i+25][\"xsaving_0\"] = para_result[8][\"xsaving_0\"]\n",
    "for i in range(0,3):\n",
    "    para_result[i+28][\"xl_g\"] = get_pop_lg(splitted_data[\"IND\"][2][i], splitted_data[\"IND\"][2][i])\n",
    "    para_result[i+28][\"xL_a\"] = splitted_data[\"IND\"][3][i]/1_000_000\n",
    "    para_result[i+28][\"xL_0\"] = splitted_data[\"IND\"][2][i][-1]/1_000_000\n",
    "    para_result[i+28][\"xg_A\"],para_result[i+28][\"xdelta_A\"] = get_gA_deltaA(splitted_data[\"IND\"][0][i])\n",
    "    para_result[i+28][\"xA_0\"] = splitted_data[\"IND\"][0][i][-1]\n",
    "    para_result[i+28][\"xK_0\"] = splitted_data[\"IND\"][1][i][-1]/1_000_000_000_000\n",
    "    para_result[i+28][\"xsigma_0\"] = raw_results[\"IND\"][\"TS_sigma\"][0][-1]/0.95\n",
    "    para_result[i+28][\"xtax\"] = para_result[10][\"xtax\"]\n",
    "    para_result[i+28][\"xmitigation_0\"] = para_result[10][\"xmitigation_0\"]\n",
    "    para_result[i+28][\"xsaving_0\"] = para_result[10][\"xsaving_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_result = {k:{} for k in raw_results.keys()}\n",
    "for i in raw_results.keys():\n",
    "    try:\n",
    "        a = raw_results[i]\n",
    "    except:\n",
    "        print(i)\n",
    "        continue\n",
    "    para_result[i][\"xl_g\"] = get_pop_lg(np.array(a[\"TS_L\"][0]), np.array(a[\"La\"]))\n",
    "    para_result[i][\"xL_a\"] = np.array(a[\"La\"])/1_000_000\n",
    "    para_result[i][\"xL_0\"] = np.array(a[\"TS_L\"][0])[-1]/1_000_000\n",
    "    para_result[i][\"xg_A\"], para_result[i][\"xdelta_A\"] = get_gA_deltaA(np.array(a[\"TS_A\"][0]))\n",
    "    para_result[i][\"xA_0\"] = np.array(a[\"TS_A\"][0])[-1]\n",
    "    para_result[i][\"xK_0\"] = np.array(a[\"TS_K\"][0])[-1]/1_000_000_000_000\n",
    "    para_result[i][\"xsigma_0\"] = np.array(a[\"TS_sigma\"][0][-1])/(1-0.05)\n",
    "    para_result[i][\"xtax\"] = np.array(a[\"tax\"])\n",
    "    para_result[i][\"xmitigation_0\"] = np.array(a[\"mitigation\"])\n",
    "    para_result[i][\"xsaving_0\"] = np.array(a[\"saving\"])\n",
    "rest_region = wld.copy()\n",
    "rest_region[\"xL_a\"] = not_covvered_convergence_pop/1_000_000\n",
    "rest_region[\"xL_0\"] = popnotcovered/1_000_000\n",
    "rest_region[\"xK_0\"] = popnotcovered*wld[\"xK_0\"]/popcovvered\n",
    "para_result[\"REST\"] = rest_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Yaml files for the merged regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:29:41.629863Z",
     "start_time": "2022-05-24T13:27:13.624Z"
    }
   },
   "outputs": [],
   "source": [
    "write_yaml_files(para_result, \"C:\\\\Users\\\\tiany\\\\Documents\\\\mila-sfdc-climate-econ\\\\region_yamls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup the dataframe used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T13:52:36.921297Z",
     "start_time": "2022-05-24T13:52:36.769296Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"backup_wbdf_2023_11_15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
