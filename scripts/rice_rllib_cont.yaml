# Copyright (c) 2022, salesforce.com, inc and MILA.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root
# or https://opensource.org/licenses/BSD-3-Clause

# Checkpoint saving setting
saving:
    metrics_log_freq: 100 # how often (in iterations) to log (and print) the metrics
    model_params_save_freq: 1000 # how often (in iterations) to save the model parameters
    basedir: "/tmp" # base folder used for saving
    name: "rice_cont" # experiment name
    tag: "experiments" # experiment tag

# Trainer settings
trainer:
    num_envs_per_worker: 1 # number of environment replicas per worker
    rollout_fragment_length: 100 # divide episodes into fragments of this many steps each during rollouts.
    train_batch_size: 2000 # total batch size used for training per iteration (across all the environments)
    num_episodes: 10000 # number of episodes to run the training for
    framework: torch # framework setting.
    # Note: RLlib supports TF as well, but our end-to-end pipeline is built for Pytorch only.
    # === Hardware Settings ===
    num_workers: 0 # number of rollout worker actors to create for parallel sampling.
    # Note: Setting the num_workers to 0 will force rollouts to be done in the trainer actor.
    # do not set to -1
    num_cpus_per_worker: 1 # do not set to -1
    num_gpus: 1 # number of GPUs to allocate to the trainer process. This can also be fractional (e.g., 0.3 GPUs).
    _enable_rl_module_api: False # sample from policy by deterministic sampling or sampling from the policy distribution
    # the output of the NN is the statistics of the distribution
    # we sample from the distribution given the statistics, or we just get the mean of the distribution
# Environment configuration
env:
    negotiation_on: False # flag to indicate whether negotiation is allowed or not
    scenario: default # key that maps to either Rice or and alternate Rice class
    num_discrete_action_levels: 1
    action_space_type: "continuous"
    dmg_function: "base"
    carbon_model: "base"
    temperature_calibration: "base"

# Policy network settings
policy:
    regions:
        vf_loss_coeff: 0.1 # loss coefficient schedule for the value function loss
        entropy_coeff_schedule: # loss coefficient schedule for the entropy loss
            # piecewise linear, specified as (timestep, value)
            - [0, 0.5]
            - [1000000, 0.1]
            - [5000000, 0.05]
        clip_grad_norm: True # flag indicating whether to clip the gradient norm or not
        max_grad_norm: 0.5 # when clip_grad_norm is True, the clip level
        gamma: 0.92 # discount factor
        lr: 0.0005 # learning rate
        model:
            custom_model: torch_linear_cont
            custom_model_config:
                fc_dims: [256, 256]
